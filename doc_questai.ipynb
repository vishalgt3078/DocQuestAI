{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishalgt3078/DocQuestAI/blob/main/doc_questai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMGCgEF4RRMg",
        "outputId": "9bdd8c51-4dac-4f47-d618-cc4a8fe9b7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
            "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading langchain_core-0.2.35-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.106-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.32-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, multidict, jsonpointer, h11, greenlet, frozenlist, async-timeout, aiohappyeyeballs, yarl, SQLAlchemy, jsonpatch, httpcore, aiosignal, httpx, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed SQLAlchemy-2.0.32 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.35 langchain-text-splitters-0.2.2 langsmith-0.1.106 multidict-6.0.5 orjson-3.10.7 tenacity-8.5.0 yarl-1.9.4\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.7.4)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.0.3-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.0.3-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.0.3 pinecone-plugin-interface-0.0.7\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.65.5)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Using cached grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.26.4)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Downloading protobuf-5.27.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
            "  Using cached grpcio-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (71.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (4.12.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.2.2)\n",
            "Downloading qdrant_client-1.11.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "Using cached grpcio-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.27.4-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: protobuf, portalocker, hyperframe, hpack, grpcio, h2, grpcio-tools, qdrant-client\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.65.5\n",
            "    Uninstalling grpcio-1.65.5:\n",
            "      Successfully uninstalled grpcio-1.65.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.27.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.4 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.66.0 grpcio-tools-1.66.0 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 portalocker-2.10.1 protobuf-5.27.4 qdrant-client-1.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client\n",
        "!pip install pypdf\n",
        "!pip install qdrant-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9hnM5P-RsIr",
        "outputId": "b308437d-f6fe-4087-b9fb-9f7be9397c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-tools 1.66.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.35)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.106)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai\n",
        "!pip install -U langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCcvB3OdRxkj",
        "outputId": "095b0d84-93ff-4277-ca0f-6364406f6769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.66.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.66.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.26.4)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.0.7)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
            "  Using cached protobuf-5.27.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (71.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (4.12.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.2.2)\n",
            "Using cached protobuf-5.27.4-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.4 which is incompatible.\n",
            "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.27.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.4 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install qdrant-client\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pinecone\n",
        "import os\n",
        "import sys\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import PointStruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hvZSuGTxR3Vv"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVpfgn3zS_i-",
        "outputId": "7bc7cde4-d253-403d-9b03-90316f3a55ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'pdfs/Word_Embeddings_and_Its_Application_in_Deep_Learni.pdf', 'page': 0}, page_content='International Journal of Innovative Technology  and Exploring Engineering (IJITEE)  \\nISSN: 2278 -3075, Volume -8 Issue -11, September  2019  \\n \\n337 Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \\nDOI: 10.35940/ijitee.K1343 .09811 19 \\n Word Embeddings and Its Application in Deep \\nLearning  \\n \\nParul  Verma , Brijesh  Khandelwal  \\n     \\n     Abstract : Word embedding in simple term can be defined as \\nrepresenting text in form of vectors.  Vector representation s of \\ntext help people in finding similarities, because contextual words \\nthat seem to appear nearby regularly  use to appear  in close \\nproximity in vector space. The motivating factor behind such \\nnumerical representation of text corpus is that it can be \\nmanipu lated arithmetically just like any other vector.  Deep \\nlearning along with neural network is not new at all, both the \\nconcepts are prevalent around the decades but there was a major \\ntailback of unavailability and accessibility of computation power. \\nDeep lea rning is now effectively being used in Natural Language \\nProcessing with the improvement in techniques  like word \\nembedding, mobile enablement and focus on attention. The paper \\nwill discuss about the two popular model of word embedding \\n(Word2Vec model) can b e used for deep learning and will also \\ncompare them. The implementation steps of Skip gram model are \\nalso discusses in the paper. The paper will also discuss \\nchallenging issues for Word2Vce model.  \\n    Keywords : Deep Learning , CBOW model, Skip -gram model,  \\nWord Embedding, Word2Vec,  \\nI. INTRODUCTION  \\nDeep learning is a new buzzword in IT industry which is \\nbasically a type of machine learning that utilizes neural \\nnetworks.  The popularity  of Deep learning approach is due \\nto its success in various applications like speech recognition, \\nimage classification, Machine Translation, Chatbots to name \\na few. People thought that its application in Natural \\nlanguage Application will also reach to the similar success \\nbenchmark. In the earlier years d ue to idiosyncrasies in NLP \\ndeep learning is not successful with the NLP approaches as \\nit is quite successful with other applications like image \\nprocessing. However in the past few years researchers have \\napplied newer deep learning techniques on NLP applications \\nand found success in it . Application of deep learning for \\nNLP has promoted use of AI to emulate human perception \\nand its usage has moved the technology  one step close to the \\nhuman abilities. Natural language processing along with AI \\ntechniques can be successfully used for the recognition and \\nclassification of , unstructured data .  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRevised Manuscript Received on September 03, 2019  \\n* Parul Verma , Assistant Professor, Dept. of IT, Amity Institute of \\nInformation Technology, Amity University Uttar Pradesh, Lucknow, U.P., \\nIndia.  \\n** Brijesh Khandelwal , Associate Professor, Dept of Comp. Sc, Amity \\nSchool of Engineering & Technology, Amity University Chh attisgarh, \\nRaipur, Chhattisgarh, India.  Deep learning is used now days to improve the effectiveness \\nof NLP applications. It has been used in various applications \\nlike Text Analytics, Voice Recognition, Image Captioning, \\nLanguage Translation and Sentiment Analysis.  With the \\nrecent improvement in the deep learning technique like \\nembeddings, a focus  on attention, mobile enablement, and \\nits appearance in the home  has been geared up for Natural \\nLanguage Processing as it had geared up for image \\nprocessing in the  past.[1] \\nA. Word Embeddings  \\nIt is most popular way of representing document vocabulary. \\nThe basic purpose of word embeddings is to capture and \\nstore the context of words with respect to document. It also \\nstores semantic and syntactic relation with other words in a \\ndocument. In computational perspective it is basically a \\nvector which stores all the contextual, semantics and \\nsyntactic relations of that word.  \\nB. Focus On Attention  \\nOne of the latest trends in Deep learning is to utilize \\nAttention Mechanism. , IlyaSutskever, now the research \\ndirector of OpenAI, mentioned that Attention Mechanisms \\nare one of the most exciting advancements, and that they are \\nhere to stay. In neural networks attention mechanism are \\nbased on the visual attention that is there in h uman beings. \\nThis technique was originally developed to improve the \\nperformance of encoder and decoder based on Recurrent \\nNeural Network used for machine translation.  \\nC. Mobile Enablement  \\nThe accessibility of internet is now days more and more \\nusing mobile devices. Mobile devices have restricted \\ncomputational power and resources. Deep learning and \\nmachine learning need expensive GPU clusters which \\nrequire lot of RAM. These all clusters can be handled easily \\non cloud environment which is not affordable by everyone. \\nHence the requirement is to make deep learning available on \\nmobile devices so that versatile applications can be \\nmanaged. There are some rec ent innovations already in use  \\n\\uf0b7 Apple in troduced a core Machine learning framework \\nwhich supports NLP based activities on iOS devices. For \\nexample – Named entity recognition and language \\nidentification.  \\n\\uf0b7 A library is developed by Baidu for mobile based deep \\nlearning capable of working on both iOS  and Android.  \\n\\uf0b7 NPE(Neural Processing Engine ) developed by \\nQualcomm mobile processors that enables deep learning \\nframework for mobile \\ndevices.  \\n \\n '),\n",
              " Document(metadata={'source': 'pdfs/Word_Embeddings_and_Its_Application_in_Deep_Learni.pdf', 'page': 1}, page_content=' \\nWord Embeddings and Its Application in Deep Learning  \\n \\n338 Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \\nDOI: 10.35940/ijitee.K1343 .09811 19 \\n II. WORD EMBEDDINGS AND DEEP LEARNING  \\nWord Embedding is the robust solution for many NLP \\nproblems. Basic usage of it is in Predictive modeling based \\non natural language processing. The basic working of word \\nembedding relies on converting space vector representation \\ninto a dense continuous vector space which enables you to \\nfind out contextual similarity between phrases an d words in \\na given document.  In general model of bag of words  every \\nword is uniquely identified which means that there is no \\ncontextual relationship between two words. For example \\nthere is a word “bank” and “finance” both will be given \\nunique id and by no direct means they can be contextually \\nconnected. By using Word Embeddings and converting \\nsparse word vectors into continuous space we can make it \\nconvenient for comparing words or phrases.  Word \\nembedding is a dense feature in a low dimensional vector \\nand i t has been proved that it is robust solution for most of \\nthe NLP issues . Word embeddings create a feature \\nrepresentation for every word establishing correlation \\namong words. Each word is represented in form of vector, \\nthat represents some features.  The ide a of word embedding \\nwas introduced by Mikolovet. al. [2] and from then it has \\nbecome a state of the art for NLP. Various researchers had \\ncontributed towards this idea and also analyzed its role in \\nthe field of deep learning. Google also tried its hands on \\nword embeddings and developed a group of algorithm \\nreferred as WordtoVec. WordtoVec algorithm is based on \\nneural network concepts and it usestwo  models – \\nA.  CBOW (Common Bag of Words) Model  \\nThe basic idea of this model is the prediction of context of \\ngiven current word within specific window. The context \\nwords are  being input at input layer and output layer \\ncontains the current word. There is a middle hidden layer \\nalso which fixes number of dimensions in which user wants \\nto represent word to be projected at output layer.  \\nIn CBOW model each word has been trained against the \\ncontext. The basic working of the model is to ask that given \\nthe set of context words what will be the suitable missing \\nword that would be likely to appear at that place. CBOW \\nmaximizes the probability o f a given word by drawing it \\nfrom its contextual words that’s why it gets difficult for \\nsome rare words to be handled. For example - Let’s take an \\nexample sentence with a given context like “When in […] \\nspeak French”. The CBOW model will tell you that the m ost \\nprobable word as per the context is “France”. Words like \\nParis, Italy will not get attention or get less attention as the \\nCBOW model is supposed to predict most probable word in \\na given context. CBOW is trained faster in comparison to the \\nskip gram and it provides better accuracy for words that are \\nfrequently used.  CBOW handles infrequent words by \\nmaking them part of a context words used to predict the \\ntarget word. Hence low probability is being assigned to the \\ninfrequent words. [3,4] \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1. CBOW model working  (Source : Reference \\nNumber 5 ) \\nB. Skip Gram Model  \\nThis model  works  by predicting  the context  words  within  a \\nfixed  size of window  by given  current  word.  The input  layer  \\ntakes  current  word  as input  and output  layer  results  into \\ncontext  words.  The hidden  layer  specifies  the dimension  in \\nwhich  user wish to project  current  word  provided  by the \\ninput  layer.  The model trains the context against the word. \\nThe basic working of the model is to result into the \\ncontextual words that are likely to appear near the given \\ninput word at the same time.  For example –Opposite to \\nCBOW if a given word is “France” the model must predict \\nthe contextual wo rds with high probability “When in speak \\nFrench”. Skip gram model does not allow low probability \\nwords to compete with the high probability words.  \\nSkip gram model is more efficient in representation of rare \\nwords or phrases and it works quite well with small amount \\nof data. Opposite to CBOW, skip -gram predicts the \\ncontextual words from a given word. In case of two words \\nplaced side -by-side both the w ords will have the same \\ntreatment in terms of minimizing the loss since each word \\nwill be treated as both the target word and context \\nword. [3,4] \\n \\n \\n \\n \\n \\n \\n \\nFigure 2. Skip Gram model working (Source : Reference \\nNumber 5 ) Input  Projection  Output  \\nw(t-2) \\nw(t-1) \\nw(t+1)  \\nw(t+2)  SUM  w(t) \\nInput  Outp ut Projection  \\nw(t) w(t-1) w(t-2) \\nw(t+1) \\nw(t+2) '),\n",
              " Document(metadata={'source': 'pdfs/Word_Embeddings_and_Its_Application_in_Deep_Learni.pdf', 'page': 2}, page_content=\"International Journal of Innovative Technology  and Exploring Engineering (IJITEE)  \\nISSN: 2278 -3075, Volume -8 Issue -11, September  2019  \\n \\n339 Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \\nDOI: 10.35940/ijitee.K1343 .09811 19 \\n III. STEPS FOR IMPLEMENTING WORD \\nEMBEDDINGS  \\nWord2Vec is considered as a revolution in the field of NLP  \\nand has given solution to various NLP based applications. \\nSkip gram model is considered as better option for the \\nimplementation of Word Embeddings. The section will \\ndiscuss basic steps for implemen tation of Skip gram model. \\nPython word2vec class is being used for implementation. [6] \\nA. Data Preparation  \\nThe very first step for implementation is to prepare data . \\nThe data that we collect from various sources for NLP \\napplications is in general unstructured and dirty. We need to \\nclean it by following various steps in order to make it ready \\nfor processing. The c leaning of unstructured text means \\nremoving stop words , punctuations and converting text to \\nlowercase. After pre -processing next step is to tokenize the \\ncorpus.  \\nLet’s take an example – “Scuba Diving and Trekking is fun \\nand exciting ” \\nAfter tokenizing and stop word removal we will have the \\ncontents like this – \\n \\n[“scuba”, “diving”,”trekking”,”fun”,”exciting”]  \\n \\nB. Hyperparameters \\u200a \\nThe second step after pre -processing is to define some \\nhyperparameters. The purpose of these hyperparameters is \\nto define few parameters like window size, embedding size, \\nepochs and learning  rate. These parameters have significant \\nrole while processing unstructured content.  \\nwindow size – The parameter need to be defined for \\ncontextual analysis. Context words are those which surround \\nthe target word. It is said that contextual words are those \\nwhich lie quite near to the target word. But there should be \\nsome parameter that will decide how near and far words are \\nconsidered for context matching, hence the need of \\nwindow_size parameter is there which decide the context \\nwindow size. If we decide win dow_size 2 this means that 2 \\nwords both left and right will be considered as context \\nwords for a given target word.  \\nEpochs – Epoch is a single pass through your entire dataset \\nwhile training. Number of training epochs need to be \\ndefined in prior.  \\n n – It is basically size of hidden layer. It typically ranges \\nfrom 100 to 300 depending on your vocabulary size.  \\nLearning_rate - The learning rate controls the amount of \\nadjustment made to the weights with respect to the loss \\ngradient.  \\nFollowing settings code you need to mention in Python – \\nsettings = {  \\n 'window_size': 2,  # context window + - center \\nword  \\n 'n': 10,   # dimensions of word \\nembeddings, also refer to size of hidden layer  \\n 'epochs': 50,  # number of training epochs  \\n 'learning_rate': 0.01  # learning rate  \\n} \\nC. Generate Training Data \\u200a \\nNext step is to generate training data which means \\nconverting corpus into one -hot encoded representation for the Word2Vec model. The first step is to initialize the object \\nof “word2vec” class and later on generate on e-hot encoded \\nrepresentation for that particular object.  \\ncorpus =”scuba diving trekking fun exciting”  \\n \\n# Initialise object  \\nw2v = word2vec()  \\n \\n# Numpyndarray with one -hot representation for \\n[target_word, context_words]  \\n \\ntraining_data = w2v.generate_training_data(settings, \\ncorpus)  \\n \\nBy calling generate_training_data() function with settings \\nand corpus as a parameter we can generate one -hot encoded \\nrepresentation of a given corpus. While training data \\nfollowing functions are called – \\n1. self.v_count  — Length of vocabulary (note that \\nvocabulary refers to th e number of unique words in the \\ncorpus)  \\n2. self.words_list  — List of words in vocabulary  \\n3. self.word_index  — Dictionary with each key as word in \\nvocabulary and value as index  \\n4. self.index_word  — Dictionary with each key as index \\nand value as word in vocabulary  \\n5. for loop to append one -hot representation for each \\ntarget and its context words \\nto training_data  using  word2onehot  function.  \\nOnce the training data is being generated we need to train our \\nmodel by generated training data.  \\nD. Model Training \\u200a \\nThe train function of word2vec class is used to train our \\nmodel – \\n# Training  \\nw2v.train(training_data)  \\n \\nThe model contains two weight matrices w1 and w2 one of \\n9X10 and 10X9 respectively. These matrices will  help in \\nback propagation error. The next step is to train our first \\nepoch by using first example by passing w_t which \\nrepresents the one -hot vector for target word to the \\nforward_pass function, Dot product between  w1 and w_t is \\nproduced  h. Then, another d ot product  using  w2 and h is \\nproduced the output layer  u. Lastly,  softmax  is run  to force \\neach element to the range of 0 and 1 to give us the \\nprobabilities for prediction before returning the vector for \\nprediction y_pred , hidden layer  h and output layer  u. \\n \\n \\n \\n \"),\n",
              " Document(metadata={'source': 'pdfs/Word_Embeddings_and_Its_Application_in_Deep_Learni.pdf', 'page': 3}, page_content=' \\nWord Embeddings and Its Application in Deep Learning  \\n \\n340 Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \\nDOI: 10.35940/ijitee.K1343 .09811 19 \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 3. Model Training Steps  \\ndefforward_pass(self,x):  \\n# x is one -hot vector for target word, shape – 9x1 \\n#Run through first matrix  (w1) to get hidden layer – \\n10x9 dot 9x1 gives 10x1  \\n        h=np.dot(self.w1.T,x)  \\n       # Dot product hidden layer with second matrix(w2) – \\n9x10 dot 10x1 gives 9x1  \\n        u=np.dot(self.w2.T,h)  \\n       # Run 1x9 through softmax to force each element to \\nrange of [0,1] - 1x8 \\ny_c=self.softmax(u)  \\n  return  y_c,h,u  \\nE. Inference\\u200a  \\nNow that we have completed training for 50 epochs, both \\nweights ( w1 and w2) are now ready to perform  inference.  \\nOnce we get the trained set of data with their weights we \\nneed to look at the vector for a particular work in the \\nvocabulary. We can draw following inferences - \\nPreparing vector for a word  \\nWith a trained set of weights, the first thing we can do  is to \\nlook at the word vector for a word in the vocabulary. We can \\nsimply do this by looking up the index of the word against \\nthe trained weight ( w1). In the following example, we look \\nup the vector for the word “ trekking ”. \\n>print(w2v.word_vec(\"trekking \")) \\n[(\\'trekking\\', array([ 0.67616509, -0.18737334,  \\n0.17229338,  0.52344708,  0.4946272 ,  \\n       -0.29398145, -0.90553722,  0.25880929,  0.72747103,  \\n0.3831458 ])) ] \\n \\nFinding similar words  \\nAnother thing we can do is to find similar words. Even \\nthough our vocabulary is sm all, we can still implement the \\nfunction  vec_sim  by computing the  cosine  similarity  \\nbetween  words.  \\n> w2v.vec_sim(\" trekking \", 3) \\n\\'trekking\\', 1.0)  (\\'diving\\', 0.4897431442336075)  \\n(\\'exciting\\', 0.029074189605917133)  \\nIV. CHALLENGES AND SOLUTIONS OF WORD \\nEMBEDDINGS FOR DEEP LEARNING  \\nWord embeddings is used to represent text numerically in \\nthe form of vectors. The purpose of generating these vectors \\nis to identify the context in which particular words are being \\nused. It is evident now that closely related words are \\nrepresented  by similar vector representation. Thus, if models \\nare trained with a single w ord, all similar vectors to this \\nword will be similarly understood by the machine.The \\nperformance of a word embedding model is measured on the \\nbasis of its performance while dealing with word analogies. \\nHence the efficient system should able to relate ‘spo rts’ with \\n‘football’ or ‘doctor’ with ‘hospital’. There are other \\nlinguistic issues that pose challenges for word embeddings \\nlike which are discuss in later sections.[7]  \\nA. Homographs  \\nThe word embedding algorithms are capable of identifying \\nsynonyms. The alg orithm justifies that there is a cosine \\nsimilarity of 0.63 between the vectors of words” house” and \\n“home”.  The vectors for love an like are also expected to be \\nsimilar, but they  have a low cosine similarity of 0.41. The \\nreason behind is that like works as a verb, adverb, \\npreposition an even noun sometimes.  These words are \\ncalled homographs and there is no means by which we can \\nidentify these identical words. The requirement is to have a \\ncommon vector for homographs representing all context of a \\nparticul ar word. That’s why the vector for  like is not as close \\nto love as expected. When put into practice, this reality can \\nsignificantly impact on the performance of ML systems \\nposing a potential problem for conversational agents and \\ntext classifiers.  \\nSolution: The solution for this issue is to train your word \\nembedding model using pre -processed text  by Part Of \\nSpeech tagger. By POS tagging in the model it is observed \\nthat verbs like and love have a cosine similarity of 0.72.  \\nB. Inflection  \\nInput Layer  np.dot  Weight 1  =  Hidden Layer  np.dot  Weight 2  = Output Layer  softmax  Prediction  \\n10x9 1x9 1x9 1x10  9x10  1x9 Word2Vec  '),\n",
              " Document(metadata={'source': 'pdfs/Word_Embeddings_and_Its_Application_in_Deep_Learni.pdf', 'page': 4}, page_content=\"International Journal of Innovative Technology  and Exploring Engineering (IJITEE)  \\nISSN: 2278 -3075, Volume -8 Issue -11, September  2019  \\n \\n341 Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \\nDOI: 10.35940/ijitee.K1343 .09811 19 \\n \\nThis is another challenging issue for word embedding. Many \\ntimes a particular word exist in their inflected form like find \\nand found (inflected) and locate and located (inflected). It is \\nobserved that find and locate share a cosine similarity of \\n0.68 whereas  found and lo cated share a los cosine similarity \\nof 0.42. That’s because  some word inflections appear less \\nfrequently  than others in certain contexts. As a result, there \\nare fewer examples of those ‘less common’ words in context \\nfor the algorithm to learn from them res ulting, therefore, in \\n‘less similar’ vectors. For all that, a far bigger issue emerges \\nwhen using languages with a greater level of inflection. No \\nmatter how large these amounts of training data are, there \\nwill not be enough examples of the ‘less common’ f orms to \\nhelp the algorithm generate useful vectors.  \\nSolution: To resolve this issue preprocessing for training of \\nword embedding models through lemmatization is done. \\nThe lemmatizer should be able to unify all different forms of  \\na words into their canonica l  form (root).  \\nC. Out of Vocabulary Words  \\nWord2Vec has another challenging issue of Out of \\nVocabulary words. If the model does not have encountered \\nany particular word earlier, it will not be in a position to \\nmake vector for the word. This is a major issue w hile \\nhandling noisy and sparse data of Twitter and making vector \\nof such OOV words is quite difficult.  \\nSolution:  The solution for this issue can be suggested in \\nmaintaining a Just In Time corpus of such words  \\nD. Lack Of Shared Representation For \\nMorphologica lly Similar Words  \\nThis is again a big challenge for word2Vec model because \\nin every language there are some words which are \\nmorphologically similar for example “care” and “careless”. \\nSuch words though morphologically similar but still it is \\nrepresented ind ividually by Word2Vec. This creates a \\nproblem for languages which are morphologically rich like \\nHindi, Arabic and German.  \\nSolution:  The words which are morphologically similar \\nshould be in close proximity in a vector space or the vector \\nshould define it by  using some common flag notation like \\nword “careless” derived from “care”.  \\nE. Pre-initialization  \\nPre-trained model in word embeddings does not go well \\nwith specialized domains. The reason behind is that \\nembeddings are trained on massive text corpus which is \\ncreated from Wikepedia and other similar sources. This is \\nthe reason behind many discrepancies. For example – word \\n‘apple’ means fruit in everyday context but it is having \\nentirely different context  in electronics field. Such \\ndifferences play an important  role while developing any \\nmodels for the analysis of critical data.  \\nSolution:  This issue can be resolved by training models on \\ndomain –specific datasets. However practically large dataset \\nfor particular domains are not available in order to draw \\nrelevant results. The basic aim is to take available pre -trained \\nword vectors and accordingly adapt them to your new \\ndomain data. The resulting representations of words are \\narguably more context -aware than the pre -trained word \\nembeddings.  \\nV. CONCLUSION  \\nWord embeddings create feature representation of words \\nand allows establishing relation among those words. Word \\nembedding creation is the basic step of working with text because computers and other computational device don’t \\nunderstand text data. Word embe dding is suggested as one \\nof the solution for deep learning and it is facilitating deep \\nlearning. The paper had discussed two popular model of \\nWord Embedding and also implementation of Skip Gram \\nmodel. Though word embedding had facilitated deep \\nlearning bu t there are many challenging issues especially in \\nthe case of morphologically rich languages. These issues \\nneed to resolved in order to draw maximum benefit of word \\nembeddings.  \\nREFERENCES  \\n1. Jacob Perkins, How Deep Learning Supercharges Natural Language \\nProcessing , TheNewsTrack, 20 Mar 2018  \\n2. Tomas Mikolov, Greg Corrado, Kai Chen & Jeffrey Dean. Efficient \\nEstimation of Word Representations in Vector Space. September \\n2013.  https://arxiv.org/pdf/1301.3781.pdf  \\n3. Elvis,Deep Learning for NLP: An Overview of Recent Trends, \\nmedium.com, August 24, 2018  \\n4. DhruvilKarani, Introduction to Word Embedding and Word2Vec, \\ntowardsdatascience, 01 September, 2018  \\n5. A Beginner's Guide to Word2Vec and  Neural Word Embeddings, \\nskymind.ai  \\n6. Dereck Chia, An implementation guide to Word2Vec using NumPy \\nand Google Sheets, towardsdatascience.com, December 6, 2018  \\n7. ParsaGhaffari , Word Embeddings and their challen ges, \\nblog.aylien.com, 15 June, 2016  \\n \\nAUTHORS PROFILE  \\nDr. Parul Verma  is working as an Assistant Professor \\nin Information Technology department in Amity \\nUniversity, Lucknow. She had completed her Ph.D. in \\nComputer Science in the year 2012 and has 12+ years \\nof teaching experience. There are more than 35 papers \\nto her credit in the International and National journals \\nand conferences. Her research interests are Natural \\nLanguage Processing, Web Mining, Word Sense Disambiguation, Semantic \\nWeb andIoT. She is a member of Review Board of several International \\nJournals. She is nomina ted as a member of Technical Program Committee \\nand Organizing Committee of many International Conferences. She is also \\na member of many International and National bodies like IAENG, IACSIT, \\nInternet Society, ACM and CSI.  \\n \\nDr. Brijesh Khandelwal has more t han 25 years of \\nencyclopedic academic experience in Computer \\nScience, Management and Insurance domain. \\nCurrently, he is working as Associate Professor and \\nHead - Department of Computer Science, Amity \\nSchool of Engineering and Technology, Amity \\nUniversity Ch hattisgarh, Raipur. Dr. Khandelwal has \\nrich & diverse experience in academia and has over 30 publications in \\nInternational/ National Journals & Conferences of repute. He is also \\nmember of Editorial/ Review Board of several Journals of repute.  \\nDr.  Khandelwal  did MCA from University of Lucknow, Lucknow in year \\n1994. In 2001, he became Sun Certified Programmer with Sun \\nMicrosystems. In 2007 he was awarded Doctorate in Philosophy in \\nAppllied Economics from University of Lucknow, Lucknow. He did MBA \\nin 2010 from Punjab Technical University. In 2010 he became licentiate in \\nLife Insurance from Insurance Institute of India, Mumbai. He also got \\nawarded Doctorate in Philosophy in the subject of Computer Science from \\nSri Venkateshwara University, Gajraula, U.P . in year 2017.  \\n \\n\")]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CjlUJWYGBN1l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yE3cJHPjTH1F"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A1y_E3sxTiTL"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSG-uQkiTmr-",
        "outputId": "7eb79c06-5fe0-4d4f-c53c-0c0d6fd255eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5mUv6NnTo7c",
        "outputId": "c68808e6-a70d-42ef-ef5a-5cbe111b0389"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'pdfs/Word_Embeddings_and_Its_Application_in_Deep_Learni.pdf', 'page': 0}, page_content='applied newer deep learning techniques on NLP applications \\nand found success in it . Application of deep learning for \\nNLP has promoted use of AI to emulate human perception \\nand its usage has moved the technology  one step close to the \\nhuman abilities. Natural language processing along with AI \\ntechniques can be successfully used for the recognition and \\nclassification of , unstructured data .  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nRevised Manuscript Received on September 03, 2019')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text_chunks[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I9vng6tZTx_t"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyABl_gJAVh-tuBiyeKN7_gyoOSG3Xx5MWw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x1U-O2zTUFC7"
      },
      "outputs": [],
      "source": [
        "embeddings=GooglePalmEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_JHmqBl1UKFe"
      },
      "outputs": [],
      "source": [
        "query_result = embeddings.embed_query(\"Hello World\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AsynYtfUOCH",
        "outputId": "8dfd242d-ac47-40f1-b544-9e32558c8334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length 768\n"
          ]
        }
      ],
      "source": [
        "print(\"Length\", len(query_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7WccMU5GVkua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5DY9Z-2VlF9",
        "outputId": "17865503-da50-443a-f8bf-82108b8e8409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collections=[CollectionDescription(name='chatpdf')]\n",
            "Collection 'chatpdf' already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "# Qdrant setup\n",
        "QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY', 'uggyPbY8nqfyObLC2Xz06TQdivDefiqws-ytCtkpMQAiTeILik-r3A')\n",
        "QDRANT_HOST = os.environ.get('QDRANT_HOST', 'localhost')  # Replace with your Qdrant host if needed\n",
        "QDRANT_PORT = os.environ.get('QDRANT_PORT', 6333)  # Default Qdrant port\n",
        "\n",
        "# Initialize Qdrant client, passing the API key directly\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"9e170198-229b-459a-bd00-21a4558560c3.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=QDRANT_API_KEY, # Pass the API key here\n",
        ")\n",
        "\n",
        "print(qdrant_client.get_collections())\n",
        "\n",
        "collection_name = \"chatpdf\"\n",
        "\n",
        "# Check if the collection exists and create it if it doesn't\n",
        "if not qdrant_client.collection_exists(collection_name): # Use collection_exists for the check\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE)  # Provide vectors_config\n",
        "    )\n",
        "    print(f\"Collection '{collection_name}' created.\")\n",
        "else:\n",
        "    print(f\"Collection '{collection_name}' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "tKrO96SLECV_",
        "outputId": "535c6fac-8474-4b78-929a-f8ce7224d192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collections=[CollectionDescription(name='chatpdf')]\n",
            "Collection 'chatpdf' already exists.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import os\n",
        "import uuid\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
        "\n",
        "# Qdrant setup\n",
        "QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY', 'uggyPbY8nqfyObLC2Xz06TQdivDefiqws-ytCtkpMQAiTeILik-r3A')\n",
        "QDRANT_URL = os.environ.get('QDRANT_URL', 'https://9e170198-229b-459a-bd00-21a4558560c3.europe-west3-0.gcp.cloud.qdrant.io:6333')  # Use the full URL\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY\n",
        ")\n",
        "\n",
        "# Check if the connection works by listing the collections\n",
        "print(qdrant_client.get_collections())\n",
        "\n",
        "collection_name = \"chatpdf\"\n",
        "\n",
        "# Check if the collection exists and create it if it doesn't\n",
        "if not qdrant_client.collection_exists(collection_name):\n",
        "    qdrant_client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
        "    )\n",
        "    print(f\"Collection '{collection_name}' created.\")\n",
        "else:\n",
        "    print(f\"Collection '{collection_name}' already exists.\")\n",
        "\n",
        "# Ensure the following imports are run in a separate cell or prior to this script if using Jupyter/Colab\n",
        "# !pip install google-generativeai\n",
        "# !pip install langchain\n",
        "\n",
        "import google.generativeai as palm\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "\n",
        "# Assuming you have initialized your palm client and embeddings object\n",
        "embeddings = GooglePalmEmbeddings()  # Ensure your GooglePalmEmbeddings object is correctly initialized\n",
        "\n",
        "# Use 'embeddings.embed_query(doc)' to get the embeddings\n",
        "documents = [t.page_content for t in text_chunks]\n",
        "vectors = [embeddings.embed_query(doc) for doc in documents]\n",
        "\n",
        "# Add documents to Qdrant collection\n",
        "points = [\n",
        "    PointStruct(\n",
        "        id=str(hash(doc)),\n",
        "        vector=vector,  # Qdrant expects a list of floats, vectors should already be in list form\n",
        "        payload={\"text\": doc}\n",
        "    )\n",
        "    for doc, vector in zip(documents, vectors)\n",
        "]\n",
        "points = [\n",
        "    PointStruct(\n",
        "        id=str(uuid.uuid4()),  # Generate a UUID string for each point\n",
        "        vector=vector,\n",
        "        payload={\"text\": doc}\n",
        "    )\n",
        "    for doc, vector in zip(documents, vectors)\n",
        "]\n",
        "\n",
        "\n",
        "# Upsert the points into the collection\n",
        "qdrant_client.upsert(\n",
        "    collection_name=collection_name,\n",
        "    points=points\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "usNAZUS4N6ad",
        "outputId": "730547e5-3d04-4f95-b465-c759934865ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for query 'what is embeddings':\n",
            "Word Embeddings and Its Application in Deep Learning  \n",
            " \n",
            "338 Published By:  \n",
            "Blue Eyes Intelligence Engineering \n",
            "& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \n",
            "DOI: 10.35940/ijitee.K1343 .09811 19 \n",
            " II. WORD EMBEDDINGS AND DEEP LEARNING  \n",
            "Word Embedding is the robust solution for many NLP \n",
            "problems. Basic usage of it is in Predictive modeling based \n",
            "on natural language processing. The basic working of word \n",
            "embedding relies on converting space vector representation\n",
            "\n",
            "Word Embeddings and Its Application in Deep Learning  \n",
            " \n",
            "338 Published By:  \n",
            "Blue Eyes Intelligence Engineering \n",
            "& Sciences Publication  Retrieval Number  K1343 0981119/2019©BEIESP  \n",
            "DOI: 10.35940/ijitee.K1343 .09811 19 \n",
            " II. WORD EMBEDDINGS AND DEEP LEARNING  \n",
            "Word Embedding is the robust solution for many NLP \n",
            "problems. Basic usage of it is in Predictive modeling based \n",
            "on natural language processing. The basic working of word \n",
            "embedding relies on converting space vector representation\n",
            "\n",
            "domain data. The resulting representations of words are \n",
            "arguably more context -aware than the pre -trained word \n",
            "embeddings.  \n",
            "V. CONCLUSION  \n",
            "Word embeddings create feature representation of words \n",
            "and allows establishing relation among those words. Word \n",
            "embedding creation is the basic step of working with text because computers and other computational device don’t \n",
            "understand text data. Word embe dding is suggested as one \n",
            "of the solution for deep learning and it is facilitating deep\n",
            "\n",
            "Results for query 'education':\n",
            "should define it by  using some common flag notation like \n",
            "word “careless” derived from “care”.  \n",
            "E. Pre-initialization  \n",
            "Pre-trained model in word embeddings does not go well \n",
            "with specialized domains. The reason behind is that \n",
            "embeddings are trained on massive text corpus which is \n",
            "created from Wikepedia and other similar sources. This is \n",
            "the reason behind many discrepancies. For example – word \n",
            "‘apple’ means fruit in everyday context but it is having\n",
            "\n",
            "should define it by  using some common flag notation like \n",
            "word “careless” derived from “care”.  \n",
            "E. Pre-initialization  \n",
            "Pre-trained model in word embeddings does not go well \n",
            "with specialized domains. The reason behind is that \n",
            "embeddings are trained on massive text corpus which is \n",
            "created from Wikepedia and other similar sources. This is \n",
            "the reason behind many discrepancies. For example – word \n",
            "‘apple’ means fruit in everyday context but it is having\n",
            "\n",
            "The basic purpose of word embeddings is to capture and \n",
            "store the context of words with respect to document. It also \n",
            "stores semantic and syntactic relation with other words in a \n",
            "document. In computational perspective it is basically a \n",
            "vector which stores all the contextual, semantics and \n",
            "syntactic relations of that word.  \n",
            "B. Focus On Attention  \n",
            "One of the latest trends in Deep learning is to utilize \n",
            "Attention Mechanism. , IlyaSutskever, now the research\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example queries\n",
        "queries = [\"what is embeddings\", \"education\"]\n",
        "\n",
        "# Process each query\n",
        "for query in queries:\n",
        "    # Convert the query to a vector\n",
        "    query_vector = embeddings.embed_query(query) # Use embed_query to generate embedding\n",
        "\n",
        "    # Perform a similarity search in the Qdrant collection\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_vector,\n",
        "        limit=3  # Retrieve top 3 most similar documents\n",
        "    )\n",
        "\n",
        "    # Extract the relevant documents\n",
        "    docs = [hit.payload['text'] for hit in search_result]\n",
        "\n",
        "    print(f\"Results for query '{query}':\")\n",
        "    for doc in docs:\n",
        "        print(doc)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--3dfm6Qf9P",
        "outputId": "fbfafe2c-1ed1-4d84-e43b-1aa44fc1e180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (1.34.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (1.8.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (3.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.35)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.106)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai\n",
        "!pip install langchain\n",
        "\n",
        "import google.generativeai as palm\n",
        "from langchain.llms import GooglePalm\n",
        "\n",
        "# Assuming you have initialized your palm client\n",
        "llm = GooglePalm(temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G4C_x2omQ_-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c5d525-c07e-4c85-a6e9-00d93eb1707d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA  # Import from langchain.chains\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "# Initialize the LLM with your parameters\n",
        "llm = GooglePalm(temperature=0.1)\n",
        "\n",
        "# Assuming you have already created and populated your Qdrant collection\n",
        "# and you have a working embeddings object\n",
        "retriever = Qdrant(\n",
        "    client=qdrant_client,  # The Qdrant client you initialized earlier\n",
        "    collection_name=collection_name,\n",
        "    embeddings=embeddings,  # Your embeddings object\n",
        "    content_payload_key=\"text\" # Add the correct key for the content field in your Qdrant documents\n",
        ").as_retriever()\n",
        "\n",
        "# Create a RetrievalQA instance using the Qdrant retriever\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Fuc84GUFRQ2T"
      },
      "outputs": [],
      "source": [
        "prompt_template  = \"\"\"\n",
        "Use the following piece of context to answer the question. Please provide a detailed response for each of the question.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in Italian\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "D1k4V2HUR9hS"
      },
      "outputs": [],
      "source": [
        "query = \"what does it mean by embeddings\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "T8yAFoJVSDUE",
        "outputId": "309d1fde-7dc5-4fdf-faf9-2d67025459bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'space vector representation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "mfrNoRGZSGBh",
        "outputId": "680c0ef8-eaed-4818-8980-13e27eb31981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt: what are the methods use in embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Word Embeddings\n",
            "Input Prompt: what is the purpose of doing this??\n",
            "Answer: To improve the effectiveness of NLP applications\n",
            "Input Prompt: please summarize this paper in short\n",
            "Answer: The paper discusses two popular models of word embedding, CBOW and Skip-gram, and their implementation. It also discusses challenging issues for Word2Vec model.\n",
            "Input Prompt: please summarize in 30 words\n",
            "Answer: Word embeddings are dense vector representations of words that capture semantic and syntactic relationships between words.\n",
            "Input Prompt: exit\n",
            "Exiting\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZAfumVMVgOR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyOcVVksvZuHlZGEMv2Sv2wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}